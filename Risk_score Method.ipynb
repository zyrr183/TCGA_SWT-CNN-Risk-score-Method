{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling1D, AveragePooling1D\n",
    "from keras.layers.convolutional import Conv1D  \n",
    "from keras.utils import np_utils\n",
    "\n",
    "import pywt\n",
    "\n",
    "# Set global constants\n",
    "_SCALE_FLAG_ = 1 \n",
    "\n",
    "\n",
    "#Read datas and labels\n",
    "ExcelFile = xlrd.open_workbook('data/Dataset_OV.xlsx')\n",
    "\n",
    "\n",
    "# Read training data\n",
    "Sheet_1 = ExcelFile.sheet_by_index(0)\n",
    "train_x_list = list()\n",
    "for i in range(Sheet_1.ncols-1):\n",
    "    tpcol = Sheet_1.col_values(i+1, 1)\n",
    "    train_x_list.append(tpcol)\n",
    "\n",
    "train_x = np.array(train_x_list)\n",
    "print(train_x.shape)\n",
    "\n",
    "# Read training label\n",
    "Sheet_1 = ExcelFile.sheet_by_index(2)\n",
    "train_y = np.array(Sheet_1.col_values(0,1))\n",
    "print(len(train_y))\n",
    "\n",
    "# Read test data\n",
    "Sheet_1 = ExcelFile.sheet_by_index(1)\n",
    "test_x_list = list()\n",
    "for i in range(Sheet_1.ncols-1):\n",
    "    tpcol = Sheet_1.col_values(i+1, 1)\n",
    "    test_x_list.append(tpcol)\n",
    "\n",
    "test_x = np.array(test_x_list)\n",
    "print(test_x.shape)\n",
    "\n",
    "# Read test label\n",
    "Sheet_1 = ExcelFile.sheet_by_index(3)\n",
    "test_y = np.array(Sheet_1.col_values(0,1))\n",
    "print(len(test_y))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------    \n",
    "train_x=train_x.T\n",
    "test_x=test_x.T\n",
    "\n",
    "#Take a wavelet transform\n",
    "waveletname='db3'                      #commonly used wavelet functions\n",
    "                                       #db1, db3, db5\n",
    "                                       #coif1, coif3, coif5\n",
    "                                       #bior3.1, bior3.3, bior3.5\n",
    "                                       #sym2, sym4, sym6\n",
    "\n",
    "signallen = train_x.shape[0]\n",
    "samplenum = train_x.shape[1]\n",
    "print(\"signallen：\"+str(signallen))\n",
    "print(\"samplenum：\"+str(samplenum))\n",
    "\n",
    "if signallen % 2 !=0:\n",
    "    train_x = np.r_[train_x, np.zeros((1,samplenum))]\n",
    "\n",
    "for i in range(samplenum):\n",
    "    \n",
    "    if np.mod(i,50) == 0:\n",
    "        print (\"Starting SWT transform: %d / %d\"%(i, samplenum))\n",
    " \n",
    "    data = train_x[:,i]\n",
    "           \n",
    "    for j in range(3):                          #The number of decomposition layers was set to 3     \n",
    "        coef = pywt.swt(data, waveletname, level=1)\n",
    "        data = coef[0][0]\n",
    "\n",
    "    if i==0:\n",
    "        train_x_wavelet=data\n",
    "    else:\n",
    "        train_x_wavelet = np.c_[train_x_wavelet, data]\n",
    "#---------------------------------------------------------------------------\n",
    "waveletname='db3'\n",
    "\n",
    "signallen = test_x.shape[0]\n",
    "samplenum = test_x.shape[1]\n",
    "print(\"signallen：\"+str(signallen))\n",
    "print(\"samplenum：\"+str(samplenum))\n",
    "\n",
    "if signallen % 2 !=0:\n",
    "    test_x = np.r_[test_x, np.zeros((1,samplenum))]\n",
    "\n",
    "for i in range(samplenum):\n",
    "    \n",
    "    if np.mod(i,50) == 0:\n",
    "        print (\"Starting SWT transform: %d / %d\"%(i, samplenum))\n",
    " \n",
    "    data = test_x[:,i]\n",
    "           \n",
    "    for j in range(3):                              #The number of decomposition layers was set to 3     \n",
    "        coef = pywt.swt(data, waveletname, level=1)\n",
    "        data = coef[0][0]\n",
    "\n",
    "    if i==0:\n",
    "        test_x_wavelet=data\n",
    "    else:\n",
    "        test_x_wavelet = np.c_[test_x_wavelet, data]\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "train_x=train_x_wavelet.T\n",
    "print(train_x.shape)\n",
    "test_x=test_x_wavelet.T\n",
    "print(test_x.shape)\n",
    "    \n",
    "# Scale data\n",
    "if _SCALE_FLAG_ == 1:\n",
    "    normalizer = preprocessing.Normalizer().fit(train_x)\n",
    "    train_x = normalizer.transform(train_x)\n",
    "    test_x = normalizer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sheet_1 = ExcelFile.sheet_by_index(0)\n",
    "train_x_gene = np.array(Sheet_1.col_values(0,1))\n",
    "print(train_x_gene)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the constants\n",
    "n_class = 2\n",
    "\n",
    "\n",
    "_CUSTOM_FILTER_NUMBER_=[64, 64]\n",
    "_CUSTOM_KERNEL_SIZE_=[4, 4]\n",
    "_CUSTOM_POOL_SIZE_=[4, 4]\n",
    "_CUSTOM_STRIDES_=2 \n",
    "_CUSTOM_DROP_RATE_=0.5 \n",
    "_CUSTOM_BATCH_SIZE_=80\n",
    "_CUSTOM_EPOCHS_=200\n",
    "_CUSTOM_SPLIT_RATE_=0.1\n",
    "_CUSTOM_OPT_FUNCTION_='RMSprop'             # Usable functions:  SGD\n",
    "                                            #                    RMSprop\n",
    "                                            #                    Adagrad\n",
    "                                            #                    Adadelta\n",
    "                                            #                    Adam\n",
    "                                            #                    Adamax\n",
    "                                            #                    Nadam\n",
    "_CUSTOM_LOSS_FUNCTION_='binary_crossentropy'    # Usable functions: mse / mean_squared_error\n",
    "                                                #                   mae / mean_absolute_error\n",
    "                                                #                   mape / mean_absolute_percentage_error\n",
    "                                                #                   msle / mean_squared_logarithmic_error\n",
    "                                                #                   squared_hinge\n",
    "                                                #                   hinge\n",
    "                                                #                   categorical_hinge\n",
    "                                                #                   binary_crossentropy\n",
    "                                                #                   logcosh\n",
    "                                                #                   categorical_crossentropy\n",
    "                                                #                   cosine_proximity\n",
    "_CUSTOM_ACT_FUNCTION_='relu'     # Usable functions:    softmax\n",
    "                                 #                      elu\n",
    "                                 #                      selu\n",
    "                                 #                      softplus\n",
    "                                 #                      softsign\n",
    "                                 #                      relu\n",
    "                                 #                      tanh\n",
    "                                 #                      sigmoid\n",
    "                                 #                      hard_sigmoid\n",
    "                                 #                      linear\n",
    "_CUSTOM_ACT_FUNCTION_OUTPUT_LAYER_='softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import os\n",
    "\n",
    "\n",
    "sum_loss=0\n",
    "sum_test_accuracy=0\n",
    "sum_train_accuracy=0\n",
    "sum_auc_train=0\n",
    "sum_auc_test=0\n",
    "\n",
    "\n",
    "test_accuracy=[]\n",
    "train_accuracy=[]\n",
    "test_auc=[]\n",
    "train_auc=[]\n",
    "\n",
    "\n",
    "\n",
    "trainning_times=100\n",
    "\n",
    "for i in range(trainning_times):\n",
    "    print(\"model training\"+str(i+1)+\":\")\n",
    "    \n",
    "    os.mkdir('result/100times_detail/OV/'+str(i+1))\n",
    "    \n",
    "    \n",
    "    #Reshape the training data and test data\n",
    "    Mtraindata = np.expand_dims(train_x, axis = 2)\n",
    "    Mtestdata = np.expand_dims(test_x, axis = 2)\n",
    "    datashape = Mtraindata.shape\n",
    "    featureLen = datashape[1]\n",
    "    input_dim = 1\n",
    "    \n",
    "    # Recompile the Class Labels\n",
    "    Ori_Vtrainlabel = train_y\n",
    "    Ori_Vtestlabel = test_y\n",
    "\n",
    "    Vtrainlabel = np_utils.to_categorical(train_y, n_class)\n",
    "    Vtestlabel = np_utils.to_categorical(test_y, n_class)\n",
    "    \n",
    "    # Model construction\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters=_CUSTOM_FILTER_NUMBER_[0],\n",
    "                 kernel_size=_CUSTOM_KERNEL_SIZE_[0],\n",
    "                 padding='same',\n",
    "                 activation=_CUSTOM_ACT_FUNCTION_,\n",
    "                 strides=_CUSTOM_STRIDES_,\n",
    "                 input_shape=(featureLen, input_dim)))\n",
    "    model.add(MaxPooling1D(pool_size = _CUSTOM_POOL_SIZE_[0], strides=None, padding='same'))\n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    model.add(Conv1D(filters=_CUSTOM_FILTER_NUMBER_[0],\n",
    "                 kernel_size=_CUSTOM_KERNEL_SIZE_[0],\n",
    "                 padding='same',\n",
    "                 activation=_CUSTOM_ACT_FUNCTION_,\n",
    "                 strides=_CUSTOM_STRIDES_)) \n",
    "    #model.add(Dropout(_CUSTOM_DROP_RATE_))\n",
    "    model.add(MaxPooling1D(pool_size = _CUSTOM_POOL_SIZE_[0], strides=None, padding='same'))\n",
    "    \"\"\" \n",
    "    \n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    #model.add(Dense(n_class, kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.001)))\n",
    "    model.add(Dense(n_class))\n",
    "    \n",
    "    model.add(Dropout(0.6))\n",
    "        \n",
    "    model.add(Activation(_CUSTOM_ACT_FUNCTION_OUTPUT_LAYER_))\n",
    "\n",
    "    model.compile(loss=_CUSTOM_LOSS_FUNCTION_, optimizer=_CUSTOM_OPT_FUNCTION_, metrics=['binary_accuracy'])\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    hist = model.fit(Mtraindata, Vtrainlabel, \n",
    "                 batch_size =_CUSTOM_BATCH_SIZE_,\n",
    "                 epochs=_CUSTOM_EPOCHS_,\n",
    "                 verbose=0,\n",
    "                 shuffle=True,\n",
    "                 validation_split=_CUSTOM_SPLIT_RATE_)\n",
    "\n",
    "    score = model.evaluate(Mtestdata, Vtestlabel, verbose=0)\n",
    "    \n",
    "    print('Test loss:', score[0])\n",
    "    \n",
    "    sum_loss+=score[0]\n",
    "    \n",
    "    \n",
    "    tr_predict = model.predict(Mtraindata)\n",
    "    test_predict = model.predict(Mtestdata)\n",
    "\n",
    "\n",
    "    train_score_y=tr_predict[:,1]\n",
    "    \n",
    "    test_score_y=test_predict[:,1]\n",
    "    \n",
    "    \n",
    "    result_df1=pd.DataFrame(train_score_y)\n",
    "    result_df1.columns=['train_score_y']\n",
    "    result_df1.to_csv('result/100times_detail/OV/'+str(i+1)+'/OV_100times_CNN_train_score_y.csv',index=False)\n",
    "    result_df2=pd.DataFrame(test_score_y)\n",
    "    result_df2.columns=['test_score_y']\n",
    "    result_df2.to_csv('result/100times_detail/OV/'+str(i+1)+'/OV_100times_CNN_test_score_y.csv',index=False)\n",
    "    \n",
    "    \n",
    "    fpr1, tpr1, thresholds1 = roc_curve(train_y.astype(int), train_score_y, pos_label=1)\n",
    "    auc1= auc(fpr1,tpr1)\n",
    "    sum_auc_train+=auc1\n",
    "    train_auc.append(auc1)\n",
    "    print(\"AUC Score (Training):  %5.4f\"%auc1)\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(test_y.astype(int), test_score_y, pos_label=1)\n",
    "    auc2= auc(fpr2,tpr2)\n",
    "    sum_auc_test+=auc2\n",
    "    test_auc.append(auc2)\n",
    "    print(\"AUC Score (Test):  %5.4f\"%auc2)\n",
    "    \n",
    "    \n",
    "    tr_predict = tr_predict.argmax(1)\n",
    "    test_predict = test_predict.argmax(1)\n",
    "    \n",
    "    result_df3=pd.DataFrame(tr_predict)\n",
    "    result_df3.columns=['tr_predict']\n",
    "    result_df3.to_csv('result/100times_detail/OV/'+str(i+1)+'/OV_100times_CNN_tr_predict.csv',index=False)\n",
    "    result_df4=pd.DataFrame(test_predict)\n",
    "    result_df4.columns=['test_predict']\n",
    "    result_df4.to_csv('result/100times_detail/OV/'+str(i+1)+'/OV_100times_CNN_test_predict.csv',index=False)\n",
    "    \n",
    "    acc_train=metrics.accuracy_score(Ori_Vtrainlabel.astype(int), tr_predict)\n",
    "    sum_train_accuracy+=acc_train\n",
    "    train_accuracy.append(acc_train)\n",
    "    print(\"ACC Score (Training):  %5.4f\"%acc_train)\n",
    "    \n",
    "    acc_test=metrics.accuracy_score(Ori_Vtestlabel.astype(int), test_predict)\n",
    "    sum_test_accuracy+=acc_test\n",
    "    test_accuracy.append(acc_test)\n",
    "    print(\"ACC Score (Test):  %5.4f\"%acc_test)\n",
    "    \n",
    "    model.save('result/100times_detail/OV/'+str(i+1)+'/OV_model.h5')\n",
    "    \n",
    "    \n",
    "    if(i==0):\n",
    "        BEST_MODEL=model\n",
    "        BEST_AUC=auc2\n",
    "        model.save(\"result/CNN_model/OV_SWT_CNN_best.h5\")\n",
    "        print('The best AUC is:'+str(BEST_AUC))\n",
    "    if(auc2>BEST_AUC):\n",
    "        BEST_MODEL=model\n",
    "        BEST_AUC=auc2\n",
    "        model.save(\"result/CNN_model/OV_SWT_CNN_best.h5\")\n",
    "        print('The best AUC is:'+str(BEST_AUC))\n",
    "    \n",
    "  \n",
    "    \n",
    "print('Average train accuracy:', sum_train_accuracy/trainning_times)\n",
    "print('Average test accuracy:', sum_test_accuracy/trainning_times)\n",
    "print('Average train auc:', sum_auc_train/trainning_times)\n",
    "print('Average test auc:', sum_auc_test/trainning_times)\n",
    "print('The best AUC is:'+str(BEST_AUC))\n",
    "\n",
    "\n",
    "accuracy_df1=pd.DataFrame(train_accuracy)\n",
    "accuracy_df1.columns=['train_accuracy']\n",
    "accuracy_df1.to_csv('result/100times/OV/OV_100times_CNN_train_accuracy.csv',index=False)\n",
    "accuracy_df2=pd.DataFrame(test_accuracy)\n",
    "accuracy_df2.columns=['test_accuracy']\n",
    "accuracy_df2.to_csv('result/100times/OV/OV_100times_CNN_test_accuracy.csv',index=False)\n",
    "\n",
    "accuracy_df3=pd.DataFrame(train_auc)\n",
    "accuracy_df3.columns=['train_auc']\n",
    "accuracy_df3.to_csv('result/100times/OV/OV_100times_CNN_train_auc.csv',index=False)\n",
    "accuracy_df4=pd.DataFrame(test_auc)\n",
    "accuracy_df4.columns=['test_auc']\n",
    "accuracy_df4.to_csv('result/100times/OV/OV_100times_CNN_test_auc.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "from keras.models import load_model\n",
    "\n",
    "test_model=load_model(\"data/OV_SWT_CNN_best.h5\")             #Change to the path that save the optimal model\n",
    "\n",
    "tr_predict = test_model.predict(Mtraindata)\n",
    "test_predict = test_model.predict(Mtestdata)\n",
    "\n",
    "train_score_y=tr_predict[:,1]\n",
    "test_score_y=test_predict[:,1]\n",
    "    \n",
    "fpr1, tpr1, thresholds1 = roc_curve(train_y.astype(int), train_score_y, pos_label=1)\n",
    "auc1= auc(fpr1,tpr1)\n",
    "print(\"AUC Score (Training):  %5.4f\"%auc1)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(test_y.astype(int), test_score_y, pos_label=1)\n",
    "auc2= auc(fpr2,tpr2)\n",
    "print(\"AUC Score (Test):  %5.4f\"%auc2)\n",
    "    \n",
    "    \n",
    "tr_predict = tr_predict.argmax(1)\n",
    "test_predict = test_predict.argmax(1)\n",
    "    \n",
    "acc_train=metrics.accuracy_score(Ori_Vtrainlabel.astype(int), tr_predict)\n",
    "print(\"ACC Score (Training):  %5.4f\"%acc_train)\n",
    "    \n",
    "acc_test=metrics.accuracy_score(Ori_Vtestlabel.astype(int), test_predict)\n",
    "print(\"ACC Score (Test):  %5.4f\"%acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(test_model.layers[0].input)\n",
    "print(test_model.layers[0].output)\n",
    "\n",
    "print(test_model.layers[1].output)\n",
    "\n",
    "print(test_model.layers[5].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "#-------------------------------pool------------------------------------------\n",
    "get_layer1_output = K.function([test_model.layers[0].input],[test_model.layers[1].output])\n",
    "layer1_output = get_layer1_output([Mtraindata])[0]\n",
    "print(layer1_output.shape)\n",
    "\n",
    "for i in range(layer1_output.shape[0]):\n",
    "    a=layer1_output[i].mean(axis=1)    \n",
    "    if i==0:\n",
    "        layer1_matrix=a\n",
    "    else:\n",
    "        layer1_matrix = np.c_[layer1_matrix, a]\n",
    "        \n",
    "print(layer1_matrix.shape)\n",
    "\n",
    "train_x=train_x.T\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------calculate B matrix----------------------------------\n",
    "temp_a=np.dot(layer1_matrix,layer1_matrix.T)\n",
    "print(temp_a)\n",
    "temp_b=np.linalg.inv(temp_a)\n",
    "print(temp_b)\n",
    "\n",
    "temp_c=np.dot(train_x,layer1_matrix.T)\n",
    "B=np.dot(temp_c,temp_b)\n",
    "print(B.shape)\n",
    "\n",
    "score=(B.sum(1))/B.shape[1]\n",
    "print(score)\n",
    "#score=np.delete(score,len(score)-1,0)     #Execute this command if the original signal length is odd\n",
    "print(score)\n",
    "print(score.shape)\n",
    "output_df=pd.DataFrame(score,index=train_x_gene)\n",
    "output_df.columns=['score']\n",
    "output_df.to_csv('result/OV/OV_pool_mean_score.csv',index=\"gene\")\n",
    "\n",
    "scores=pd.DataFrame(score,columns=['score'],index=train_x_gene)\n",
    "geneSorted=scores.sort_values(by=['score'],ascending=False)\n",
    "print(geneSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = pd.read_excel('data/Dataset_OV.xlsx','train_x',index_col=\"gene\")\n",
    "\n",
    "temp_output_topGene=[]\n",
    "\n",
    "geneList=geneSorted.index\n",
    "print(geneList)\n",
    "\n",
    "N=1000\n",
    "for i in range(N):\n",
    "    if(geneList[i] not in train_x.index):\n",
    "        temp_output_topGene.append(np.zeros((1,len(train_x.columns))))\n",
    "        continue\n",
    "    temp=train_x.loc[geneList[i]]\n",
    "    temp_output_topGene.append(np.array(temp))\n",
    "\n",
    "output_topGene=pd.DataFrame(temp_output_topGene,columns=train_x.columns,index=geneList[0:1000:1])\n",
    "\n",
    "\n",
    "output_topGene=output_topGene.T\n",
    "print(output_topGene)\n",
    "output_topGene.to_csv('result/OV/OV_top1000Gene.csv',index=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the scores of each sample after the cox regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cox_result=pd.read_csv('data/OV_clinical_MultipleResult_0.05.csv',sep=',',index_col=\"Characteristics\")   #the gene list after the Multivariable Cox regression(p < 0.05) \n",
    "print(cox_result)\n",
    "cox_gene=cox_result.index\n",
    "print(cox_gene)\n",
    "\n",
    "\n",
    "train_x = pd.read_excel('data/Dataset_OV.xlsx','train_x',index_col=\"gene\")\n",
    "test_x = pd.read_excel('data/Dataset_OV.xlsx','test_x',index_col=\"gene\")\n",
    "train_x_sample=train_x.columns \n",
    "test_x_sample=test_x.columns\n",
    "\n",
    "train_sample_score=np.zeros((1,len(train_x_sample)))\n",
    "test_sample_score=np.zeros((1,len(test_x_sample))) \n",
    "\n",
    "for i in range(len(cox_gene)):\n",
    "    temp=train_x.loc[cox_gene[i]]\n",
    "    train_sample_score+=np.array(temp)*cox_result.loc[cox_gene[i]]['Coefficient']\n",
    "    \n",
    "for i in range(len(cox_gene)):\n",
    "    temp=test_x.loc[cox_gene[i]]\n",
    "    test_sample_score+=np.array(temp)*cox_result.loc[cox_gene[i]]['Coefficient']\n",
    "\n",
    "train_sample_score=train_sample_score.T\n",
    "print(train_sample_score.shape)\n",
    "test_sample_score=test_sample_score.T\n",
    "print(test_sample_score.shape)\n",
    "    \n",
    "\n",
    "train_sample_score=pd.DataFrame(train_sample_score,index=train_x_sample)\n",
    "test_sample_score=pd.DataFrame(test_sample_score,index=test_x_sample)\n",
    "\n",
    "    \n",
    "writer=pd.ExcelWriter('result/OV_cox0.05_Sample_score.xlsx')\n",
    "\n",
    "\n",
    "train_sample_score.to_excel(writer,sheet_name='train_x',index_label=\"sample\")\n",
    "test_sample_score.to_excel(writer,sheet_name='test_x',index_label=\"sample\")\n",
    "\n",
    "writer.save()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
