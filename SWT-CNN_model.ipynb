{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling1D, AveragePooling1D\n",
    "from keras.layers.convolutional import Conv1D  \n",
    "from keras.utils import np_utils\n",
    "\n",
    "import pywt\n",
    "\n",
    "# Set global constants\n",
    "_SCALE_FLAG_ = 1 \n",
    "\n",
    "\n",
    "#Read datas and labels\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "x=pd.read_csv('data/OV_finalMatrix.tsv',sep='\\t',index_col=\"gene\")\n",
    "x=np.array(x)\n",
    "print(x.shape)          #read datas\n",
    "\n",
    "f=open(\"data/OV_clinical.csv\")\n",
    "for line in f:\n",
    "    temp=line.split(\",\")\n",
    "    y.append(temp[1].strip())\n",
    "\n",
    "y=np.delete(y,0,0)\n",
    "y=np.array(y)\n",
    "print(len(y))          #read labels\n",
    "    \n",
    "\n",
    "\n",
    "#Take a wavelet transform\n",
    "waveletname='db3'                      #commonly used wavelet functions\n",
    "                                       #db1, db3, db5\n",
    "                                       #coif1, coif3, coif5\n",
    "                                       #bior3.1, bior3.3, bior3.5\n",
    "                                       #sym2, sym4, sym6\n",
    "\n",
    "signallen = x.shape[0]\n",
    "samplenum = x.shape[1]\n",
    "print(\"signallen：\"+str(signallen))\n",
    "print(\"samplenum：\"+str(samplenum))\n",
    "\n",
    "if signallen % 2 !=0:\n",
    "    x = np.r_[x, np.zeros((1,samplenum))]\n",
    "\n",
    "for i in range(samplenum):\n",
    "    \n",
    "    if np.mod(i,50) == 0:\n",
    "        print (\"Starting SWT transform: %d / %d\"%(i, samplenum))\n",
    " \n",
    "    data = x[:,i]\n",
    "           \n",
    "    for j in range(3):                  #The number of decomposition layers was set to 3                       \n",
    "        coef = pywt.swt(data, waveletname, level=1)\n",
    "        data = coef[0][0]\n",
    "\n",
    "    if i==0:\n",
    "        x_wavelet=data\n",
    "    else:\n",
    "        x_wavelet = np.c_[x_wavelet, data]\n",
    "\n",
    "\n",
    "x=x_wavelet.T\n",
    "print(x.shape)\n",
    "    \n",
    "# Scale data\n",
    "if _SCALE_FLAG_ == 1:\n",
    "    normalizer = preprocessing.Normalizer().fit(x)\n",
    "    x = normalizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the constants\n",
    "n_class = 2\n",
    "\n",
    "\n",
    "_CUSTOM_FILTER_NUMBER_=[64, 64]\n",
    "_CUSTOM_KERNEL_SIZE_=[4, 4]\n",
    "_CUSTOM_POOL_SIZE_=[4, 4]\n",
    "_CUSTOM_STRIDES_=2  \n",
    "_CUSTOM_DROP_RATE_=0.5 \n",
    "_CUSTOM_BATCH_SIZE_=80\n",
    "_CUSTOM_EPOCHS_=200\n",
    "_CUSTOM_SPLIT_RATE_=0.1\n",
    "_CUSTOM_OPT_FUNCTION_='RMSprop'             # Usable functions:  SGD\n",
    "#优化器                                      #                    RMSprop\n",
    "                                            #                    Adagrad\n",
    "                                            #                    Adadelta\n",
    "                                            #                    Adam\n",
    "                                            #                    Adamax\n",
    "                                            #                    Nadam\n",
    "_CUSTOM_LOSS_FUNCTION_='binary_crossentropy'    # Usable functions: mse / mean_squared_error\n",
    "#目标函数                                        #                   mae / mean_absolute_error\n",
    "                                                #                   mape / mean_absolute_percentage_error\n",
    "                                                #                   msle / mean_squared_logarithmic_error\n",
    "                                                #                   squared_hinge\n",
    "                                                #                   hinge\n",
    "                                                #                   categorical_hinge\n",
    "                                                #                   binary_crossentropy\n",
    "                                                #                   logcosh\n",
    "                                                #                   categorical_crossentropy\n",
    "                                                #                   cosine_proximity\n",
    "_CUSTOM_ACT_FUNCTION_='relu'     # Usable functions:    softmax\n",
    "#指定激活函数                     #                      elu\n",
    "                                 #                      selu\n",
    "                                 #                      softplus\n",
    "                                 #                      softsign\n",
    "                                 #                      relu\n",
    "                                 #                      tanh\n",
    "                                 #                      sigmoid\n",
    "                                 #                      hard_sigmoid\n",
    "                                 #                      linear\n",
    "_CUSTOM_ACT_FUNCTION_OUTPUT_LAYER_='softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "sum_loss=0\n",
    "sum_test_accuracy=0\n",
    "sum_train_accuracy=0\n",
    "sum_train_mcc=0\n",
    "sum_test_mcc=0\n",
    "sum_precision_train=0\n",
    "sum_precision_test=0\n",
    "sum_recall_train=0\n",
    "sum_recall_test=0\n",
    "sum_f1_train=0\n",
    "sum_f1_test=0\n",
    "sum_auc_train=0\n",
    "sum_auc_test=0\n",
    "sum_se_train=0\n",
    "sum_se_test=0\n",
    "sum_sp_train=0\n",
    "sum_sp_test=0\n",
    "\n",
    "\n",
    "trainning_times=100\n",
    "\n",
    "for i in range(trainning_times):\n",
    "    print(\"model training\"+str(i+1)+\":\")\n",
    "    \n",
    "    \n",
    "    #Split train set and test set\n",
    "    train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.3,random_state=None,stratify=y)\n",
    "    \n",
    "    \n",
    "    #Reshape the training data and test data\n",
    "    Mtraindata = np.expand_dims(train_x, axis = 2)\n",
    "    Mtestdata = np.expand_dims(test_x, axis = 2)\n",
    "    datashape = Mtraindata.shape\n",
    "    featureLen = datashape[1]\n",
    "    input_dim = 1\n",
    "    \n",
    "    # Recompile the Class Labels\n",
    "    Ori_Vtrainlabel = train_y\n",
    "    Ori_Vtestlabel = test_y\n",
    "\n",
    "    \n",
    "    Vtrainlabel = np_utils.to_categorical(train_y, n_class)\n",
    "    Vtestlabel = np_utils.to_categorical(test_y, n_class)\n",
    "    \n",
    "    # Model construction\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters=_CUSTOM_FILTER_NUMBER_[0], \n",
    "                 kernel_size=_CUSTOM_KERNEL_SIZE_[0],\n",
    "                 padding='same',\n",
    "                 activation=_CUSTOM_ACT_FUNCTION_,\n",
    "                 strides=_CUSTOM_STRIDES_,\n",
    "                 input_shape=(featureLen, input_dim)))  \n",
    "    model.add(MaxPooling1D(pool_size = _CUSTOM_POOL_SIZE_[0], strides=None, padding='same')) \n",
    "          \n",
    "    \"\"\" \n",
    "    model.add(Conv1D(filters=_CUSTOM_FILTER_NUMBER_[0],\n",
    "                 kernel_size=_CUSTOM_KERNEL_SIZE_[0],\n",
    "                 padding='same',                        \n",
    "                 activation=_CUSTOM_ACT_FUNCTION_,      \n",
    "                 strides=_CUSTOM_STRIDES_))\n",
    "    #model.add(Dropout(_CUSTOM_DROP_RATE_))\n",
    "    model.add(MaxPooling1D(pool_size = _CUSTOM_POOL_SIZE_[0], strides=None, padding='same'))\n",
    "    \"\"\" \n",
    "    \n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    #model.add(Dense(n_class, kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.001)))    \n",
    "    model.add(Dense(n_class))\n",
    "    \n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    model.add(Activation(_CUSTOM_ACT_FUNCTION_OUTPUT_LAYER_))\n",
    "\n",
    "    model.compile(loss=_CUSTOM_LOSS_FUNCTION_, optimizer=_CUSTOM_OPT_FUNCTION_, metrics=['binary_accuracy'])\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    #训练模型\n",
    "    hist = model.fit(Mtraindata, Vtrainlabel, \n",
    "                 batch_size =_CUSTOM_BATCH_SIZE_,\n",
    "                 epochs=_CUSTOM_EPOCHS_,      \n",
    "                 verbose=0,       \n",
    "                 shuffle=True,    \n",
    "                 validation_split=_CUSTOM_SPLIT_RATE_)\n",
    "\n",
    "    score = model.evaluate(Mtestdata, Vtestlabel, verbose=0)\n",
    "    \n",
    "    print('Test loss:', score[0])\n",
    "    \n",
    "    sum_loss+=score[0]\n",
    "    \n",
    "    tr_predict = model.predict(Mtraindata)\n",
    "    test_predict = model.predict(Mtestdata)\n",
    "\n",
    "    train_score_y=tr_predict[:,1]\n",
    "    test_score_y=test_predict[:,1]\n",
    "    \n",
    "    \n",
    "    fpr1, tpr1, thresholds1 = roc_curve(train_y.astype(int), train_score_y, pos_label=1)\n",
    "    auc1= auc(fpr1,tpr1)\n",
    "    sum_auc_train+=auc1\n",
    "    print(\"AUC Score (Training):  %5.4f\"%auc1)\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(test_y.astype(int), test_score_y, pos_label=1)\n",
    "    auc2= auc(fpr2,tpr2)\n",
    "    sum_auc_test+=auc2\n",
    "    print(\"AUC Score (Test):  %5.4f\"%auc2)\n",
    "    \n",
    "    \n",
    "    tr_predict = tr_predict.argmax(1)\n",
    "    test_predict = test_predict.argmax(1)\n",
    "    \n",
    "    acc_train=metrics.accuracy_score(Ori_Vtrainlabel.astype(int), tr_predict)\n",
    "    sum_train_accuracy+=acc_train\n",
    "    print(\"ACC Score (Training):  %5.4f\"%acc_train)\n",
    "    \n",
    "    acc_test=metrics.accuracy_score(Ori_Vtestlabel.astype(int), test_predict)\n",
    "    sum_test_accuracy+=acc_test\n",
    "    print(\"ACC Score (Test):  %5.4f\"%acc_test)\n",
    "\n",
    "\n",
    "    tn1, fp1, fn1, tp1 = metrics.confusion_matrix(Ori_Vtrainlabel.astype(int), tr_predict, labels=[0, 1]).ravel()\n",
    "    #print(\"Training: (TP, FP, TN, FN):  %d, %d, %d, %d\"%(tp1, fp1, tn1, fn1))\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(Ori_Vtestlabel.astype(int), test_predict, labels=[0, 1]).ravel()\n",
    "    #print(\"Test: (TP, FP, TN, FN):  %d, %d, %d, %d\"%(tp, fp, tn, fn))\n",
    "    \n",
    "    SE_train=tp1/ float(tp1+ fn1)\n",
    "    SE_test=tp/ float(tp+ fn)\n",
    "    sum_se_train+=SE_train\n",
    "    sum_se_test+=SE_test\n",
    "    #print(\"SE (Training):  %5.4f\"%SE_train)\n",
    "    #print(\"SE (Test):  %5.4f\"%SE_test)\n",
    "    SP_train= tn1 / float(tn1 + fp1)\n",
    "    SP_test= tn / float(tn + fp)\n",
    "    sum_sp_train+=SP_train\n",
    "    sum_sp_test+=SP_test\n",
    "    #print(\"SP (Training):  %5.4f\"%SP_train)\n",
    "    #print(\"SP (Test):  %5.4f\"%SP_test)\n",
    "    \n",
    "    mcc_train = metrics.matthews_corrcoef(Ori_Vtrainlabel.astype(int), tr_predict)\n",
    "    sum_train_mcc+=mcc_train\n",
    "    #print (\"MCC Score (Training):  %5.4f\"%mcc_train)\n",
    "    mcc_test = metrics.matthews_corrcoef(Ori_Vtestlabel.astype(int), test_predict)\n",
    "    sum_test_mcc+=mcc_test\n",
    "    #print (\"MCC Score (Test):  %5.4f\"%mcc_test)\n",
    "    \n",
    "    precision_train=precision_score(Ori_Vtrainlabel.astype(int), tr_predict, average='weighted')\n",
    "    precision_test=precision_score(Ori_Vtestlabel.astype(int), test_predict, average='weighted')\n",
    "    sum_precision_train+=precision_train\n",
    "    sum_precision_test+=precision_test\n",
    "\n",
    "    #print(\"precision_train:\",precision_train)\n",
    "    #print(\"precision_test:\",precision_test)\n",
    "\n",
    "    recall_train=recall_score(Ori_Vtrainlabel.astype(int), tr_predict, average='weighted')\n",
    "    recall_test=recall_score(Ori_Vtestlabel.astype(int), test_predict, average='weighted')\n",
    "    sum_recall_train+=recall_train\n",
    "    sum_recall_test+=recall_test\n",
    "    #print(\"recall_train:\",recall_train)\n",
    "    #print(\"recall_test:\",recall_test)\n",
    "\n",
    "    #计算F1值\n",
    "    f1_train=f1_score(Ori_Vtrainlabel.astype(int), tr_predict, average='weighted')\n",
    "    f1_test=f1_score(Ori_Vtestlabel.astype(int), test_predict, average='weighted')\n",
    "    sum_f1_train+=f1_train\n",
    "    sum_f1_test+=f1_test\n",
    "    #print(\"f1_train:\",f1_train)\n",
    "    #print(\"f1_test:\",f1_test)\n",
    "    \n",
    "print('Average train accuracy:', sum_train_accuracy/trainning_times)\n",
    "print('Average test accuracy:', sum_test_accuracy/trainning_times)\n",
    "print('Average train auc:', sum_auc_train/trainning_times)\n",
    "print('Average test auc:', sum_auc_test/trainning_times)\n",
    "print('Average train SE:', sum_se_train/trainning_times)\n",
    "print('Average test SE:', sum_se_test/trainning_times)\n",
    "print('Average train SP:', sum_sp_train/trainning_times)\n",
    "print('Average test SP:', sum_sp_test/trainning_times)\n",
    "print('Average mcc_train:',sum_train_mcc/trainning_times)\n",
    "print('Average mcc_test:',sum_test_mcc/trainning_times)\n",
    "print('Average precision_train:',sum_precision_train/trainning_times)\n",
    "print('Average precision_test:',sum_precision_test/trainning_times)\n",
    "print('Average recall_train:',sum_recall_train/trainning_times)\n",
    "print('Average recall_test:',sum_recall_test/trainning_times)\n",
    "print('Average f1_train:',sum_f1_train/trainning_times)\n",
    "print('Average f1_test:',sum_f1_test/trainning_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
